
import asyncio
import json
import os
import aiohttp
import pandas as pd
from .crawler import TikiFetcher
from .utils import setup_logger

class TikiPipeline:
    def __init__(self, input_file, output_dir="data", log_dir="logs"):
        self.input_file = input_file
        self.output_dir = output_dir
        self.log_dir = log_dir
        
        # Táº¡o thÆ° má»¥c output náº¿u chÆ°a cÃ³
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        self.logger = setup_logger(log_dir)
        self.fetcher = TikiFetcher(self.logger)
        self.batch_size = 1000

    def get_completed_ids(self):
        """
        QuÃ©t thÆ° má»¥c data xÃ¡c Ä‘á»‹nh (Resume) cÃ¡c ID Ä‘Ã£ táº£i xong.
        """
        completed_ids = set()
        if not os.path.exists(self.output_dir):
            return completed_ids
            
        files = [f for f in os.listdir(self.output_dir) if f.endswith('.json')]
        for file in files:
            path = os.path.join(self.output_dir, file)
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for item in data:
                        completed_ids.add(str(item['id'])) # LÆ°u dáº¡ng string Ä‘á»ƒ so sÃ¡nh nháº¥t quÃ¡n
            except Exception:
                continue
                
        self.logger.info(f"ğŸ”„ RESUME: TÃ¬m tháº¥y {len(completed_ids)} sáº£n pháº©m Ä‘Ã£ táº£i trÆ°á»›c Ä‘Ã³.")
        return completed_ids

    def load_pending_ids(self):
        """
        Äá»c file CSV vÃ  lá»c ra cÃ¡c ID chÆ°a táº£i.
        """
        try:
            df = pd.read_csv(self.input_file, dtype={'id': str})
            all_ids = set(df['id'].dropna().unique())
            completed_ids = self.get_completed_ids()
            
            pending_ids = list(all_ids - completed_ids)
            self.logger.info(f"Tá»•ng ID: {len(all_ids)} | ÄÃ£ xong: {len(completed_ids)} | CÃ²n láº¡i: {len(pending_ids)}")
            return pending_ids
        except Exception as e:
            self.logger.critical(f"FATAL: KhÃ´ng thá»ƒ Ä‘á»c file input CSV: {str(e)}")
            return []

    async def process_batch(self, session, batch_ids, batch_index):
        """
        Xá»­ lÃ½ song song má»™t lÃ´ (batch) 1000 IDs.
        """
        tasks = [self.fetcher.fetch_product(session, pid) for pid in batch_ids]
        results = await asyncio.gather(*tasks)
        
        # Lá»c káº¿t quáº£ None (do lá»—i 404 hoáº·c max retries)
        valid_results = [r for r in results if r is not None]
        
        if valid_results:
            self.save_batch(valid_results, batch_index)
            
        return len(valid_results)

    def save_batch(self, data, batch_index):
        """
        LÆ°u káº¿t quáº£ ra file JSON.
        """
        filename = f"products_batch_{batch_index}.json"
        filepath = os.path.join(self.output_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            self.logger.info(f"ğŸ’¾ ÄÃ£ lÆ°u batch {batch_index}: {len(data)} sáº£n pháº©m -> {filename}")
        except Exception as e:
            self.logger.error(f"WRITE ERROR: KhÃ´ng thá»ƒ lÆ°u file {filename}: {str(e)}")

    async def run(self):
        """
        HÃ m chÃ­nh Ä‘iá»u phá»‘i toÃ n bá»™ quy trÃ¬nh.
        """
        pending_ids = self.load_pending_ids()
        if not pending_ids:
            self.logger.info("ğŸ‰ Táº¥t cáº£ dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c táº£i xong!")
            return

        total_pending = len(pending_ids)
        self.logger.info("ğŸš€ Báº¯t Ä‘áº§u tiáº¿n trÃ¬nh crawling...")

        async with aiohttp.ClientSession() as session:
            # Chia nhá» thÃ nh cÃ¡c batch
            for i in range(0, total_pending, self.batch_size):
                batch_ids = pending_ids[i : i + self.batch_size]
                batch_index = (i // self.batch_size) + 1 # ÄÃ¡nh sá»‘ batch tÆ°Æ¡ng Ä‘á»‘i cho láº§n cháº¡y nÃ y
                
                # Äá»ƒ trÃ¡nh trÃ¹ng láº·p tÃªn file khi resume, ta nÃªn Ä‘Ã¡nh sá»‘ batch dá»±a theo timestamp hoáº·c UUID 
                # Tuy nhiÃªn user yÃªu cáº§u Ä‘Æ¡n giáº£n, ta dÃ¹ng index + timestamp.
                # á» Ä‘Ã¢y Ä‘á»ƒ Ä‘Æ¡n giáº£n vÃ  dá»… kiá»ƒm tra, ta dÃ¹ng index cá»§a máº£ng pending hiá»‡n táº¡i
                # (LÆ°u Ã½: Náº¿u resume nhiá»u láº§n sáº½ sinh ra nhiá»u file nhá» láº», post-process cÃ³ thá»ƒ merge sau).
                
                # CÃ¡ch tá»‘t hÆ¡n: DÃ¹ng UUID cho filename Ä‘á»ƒ an toÃ n tuyá»‡t Ä‘á»‘i
                import uuid
                safe_batch_name = f"{uuid.uuid4().hex[:8]}" 
                
                self.logger.info(f"Äang xá»­ lÃ½ Batch {i//self.batch_size + 1} ({len(batch_ids)} items)...")
                
                await self.process_batch(session, batch_ids, safe_batch_name)
        
        self.logger.info("âœ… HOÃ€N THÃ€NH TOÃ€N Bá»˜ CÃ”NG VIá»†C.")
