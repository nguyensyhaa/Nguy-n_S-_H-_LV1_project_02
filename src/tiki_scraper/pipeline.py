
import asyncio
import json
import os
import glob
import aiohttp
import pandas as pd
from .crawler import TikiFetcher
from .utils import setup_logger, send_discord_webhook
from dotenv import load_dotenv

load_dotenv()
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")

import time

class TikiPipeline:
    def __init__(self, input_file, output_dir="data", log_dir="logs"):
        self.input_file = input_file
        self.output_dir = output_dir
        self.log_dir = log_dir
        
        # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        self.logger = setup_logger(log_dir)
        self.fetcher = TikiFetcher(self.logger)
        self.batch_size = 1000

    def get_completed_ids(self):
        """
        Qu√©t th∆∞ m·ª•c data x√°c ƒë·ªãnh (Resume) c√°c ID ƒë√£ t·∫£i xong.
        """
        completed_ids = set()
        if not os.path.exists(self.output_dir):
            return completed_ids
            
        files = [f for f in os.listdir(self.output_dir) if f.endswith('.json')]
        for file in files:
            path = os.path.join(self.output_dir, file)
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for item in data:
                        completed_ids.add(str(item['id'])) # L∆∞u d·∫°ng string ƒë·ªÉ so s√°nh nh·∫•t qu√°n
            except Exception:
                continue
                
        self.logger.info(f"üîÑ RESUME: T√¨m th·∫•y {len(completed_ids)} s·∫£n ph·∫©m ƒë√£ t·∫£i tr∆∞·ªõc ƒë√≥.")
        return completed_ids

    def load_pending_ids(self):
        """
        ƒê·ªçc file CSV v√† l·ªçc ra c√°c ID ch∆∞a t·∫£i.
        """
        try:
            df = pd.read_csv(self.input_file, dtype={'id': str})
            all_ids = set(df['id'].dropna().unique())
            completed_ids = self.get_completed_ids()
            
            pending_ids = list(all_ids - completed_ids)
            self.logger.info(f"T·ªïng ID: {len(all_ids)} | ƒê√£ xong: {len(completed_ids)} | C√≤n l·∫°i: {len(pending_ids)}")
            return pending_ids, len(all_ids), len(completed_ids)
        except Exception as e:
            self.logger.critical(f"FATAL: Kh√¥ng th·ªÉ ƒë·ªçc file input CSV: {str(e)}")
            return [], 0, 0

    async def process_batch(self, session, batch_ids, batch_index):
        """
        X·ª≠ l√Ω song song m·ªôt l√¥ (batch) 1000 IDs.
        """
        tasks = [self.fetcher.fetch_product(session, pid) for pid in batch_ids]
        results = await asyncio.gather(*tasks)
        
        # L·ªçc k·∫øt qu·∫£ None (do l·ªói 404 ho·∫∑c max retries)
        valid_results = [r for r in results if r is not None]
        
        if valid_results:
            self.save_batch(valid_results, batch_index)
            
        return len(valid_results)

    def save_batch(self, data, batch_index):
        """
        L∆∞u k·∫øt qu·∫£ ra file JSON.
        """
        filename = f"products_batch_{batch_index}.json"
        filepath = os.path.join(self.output_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            msg = f"üíæ ƒê√£ l∆∞u batch {batch_index}: {len(data)} sxp -> {filename}"
            self.logger.info(msg)
            # Kh√¥ng g·ª≠i notify ·ªü ƒë√¢y ƒë·ªÉ tr√°nh spam qu√° nhi·ªÅu, ho·∫∑c g·ª≠i d·∫°ng silent
        except Exception as e:
            self.logger.error(f"WRITE ERROR: Kh√¥ng th·ªÉ l∆∞u file {filename}: {str(e)}")

    def _get_temp_file_path(self):
        return os.path.join(self.output_dir, "temp_buffer.jsonl")

    def _load_buffer_from_disk(self):
        """Kh√¥i ph·ª•c d·ªØ li·ªáu t·ª´ file nh√°p (n·∫øu c√≥) sau khi b·ªã crash"""
        buffer = []
        temp_path = self._get_temp_file_path()
        if os.path.exists(temp_path):
            try:
                with open(temp_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            buffer.append(json.loads(line))
                self.logger.info(f"‚ù§Ô∏è PH·ª§C H·ªíI D·ªÆ LI·ªÜU: T√¨m th·∫•y {len(buffer)} s·∫£n ph·∫©m trong file nh√°p!")
            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è L·ªói ƒë·ªçc file nh√°p: {e}")
        return buffer

    def _append_to_temp_file(self, item):
        """Ghi ngay l·∫≠p t·ª©c 1 item xu·ªëng ƒëƒ©a (WAL)"""
        try:
            with open(self._get_temp_file_path(), 'a', encoding='utf-8') as f:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        except Exception as e:
            self.logger.error(f"‚ùå WAL ERROR: Kh√¥ng th·ªÉ ghi file nh√°p: {e}")

    def _rewrite_temp_file(self, buffer):
        """Ghi l·∫°i file nh√°p m·ªõi (d√πng sau khi ƒë√£ c·∫Øt b·ªõt 1000 item)"""
        try:
            with open(self._get_temp_file_path(), 'w', encoding='utf-8') as f:
                for item in buffer:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
        except Exception as e:
            self.logger.error(f"‚ùå WAL ERROR: Kh√¥ng th·ªÉ l√†m m·ªõi file nh√°p: {e}")

    async def run(self):
        """
        H√†m ch√≠nh ƒëi·ªÅu ph·ªëi to√†n b·ªô quy tr√¨nh.
        """
        pending_ids, total_source, processed_count = self.load_pending_ids()
        if not pending_ids:
            self.logger.info("üéâ T·∫•t c·∫£ d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c t·∫£i xong!")
            # await send_discord_webhook(DISCORD_WEBHOOK_URL, "üéâ **Tiki Crawler**: T·∫•t c·∫£ d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c t·∫£i xong!")
            return

        total_pending = len(pending_ids)
        
        # --- RICH START NOTIFICATION ---
        start_time = time.time()
        
        if DISCORD_WEBHOOK_URL:
            # G·ª≠i Embed m·ªü m√†n (Blue)
            embed_start = {
                "title": "üöÄ TIKI CRAWLER: KH·ªûI ƒê·ªòNG!",
                "description": f"B·∫Øt ƒë·∫ßu chi·∫øn d·ªãch l·∫•y **{total_source:,}** s·∫£n ph·∫©m.",
                "color": 3447003, # Blue
                "fields": [
                    {"name": "üì¶ T·ªïng Input", "value": f"{total_source:,}", "inline": True},
                    {"name": "‚úÖ ƒê√£ xong", "value": f"{processed_count:,}", "inline": True},
                    {"name": "‚è≥ C√≤n l·∫°i", "value": f"{total_pending:,}", "inline": True},
                ],
                "footer": {"text": "Tiki Scraper `v2.0` | Mode: Async/WAL"}
            }
            await send_discord_webhook(DISCORD_WEBHOOK_URL, embed=embed_start)

        self.logger.info(f"üöÄ Tiki Crawler: B·∫Øt ƒë·∫ßu ch·∫°y! C√≤n l·∫°i {total_pending} ID.")

        # 1. Kh√¥i ph·ª•c buffer t·ª´ ƒëƒ©a (Crash Recovery)
        result_buffer = self._load_buffer_from_disk()
        
        batch_counter = 1
        existing_files = glob.glob(os.path.join(self.output_dir, "products_batch_*.json"))
        if existing_files:
            batch_counter = len(existing_files) + 1

        initial_batch = batch_counter

        async with aiohttp.ClientSession() as session:
            input_chunk_size = 200 
            
            try:
                for i in range(0, total_pending, input_chunk_size):
                    chunk_ids = pending_ids[i : i + input_chunk_size]
                    self.logger.info(f"ƒêang x·ª≠ l√Ω chunk input {i}/{total_pending}...")
                    
                    # Fetch song song
                    tasks = [self.fetcher.fetch_product(session, pid) for pid in chunk_ids]
                    results = await asyncio.gather(*tasks)
                    
                    for idx, res in enumerate(results):
                        pid = chunk_ids[idx]
                        if res:
                            # Check tr√πng trong buffer ƒë·ªÉ tr√°nh duplicate khi resume ch·ªìng ch√©o
                            if not any(d['id'] == res['id'] for d in result_buffer):
                                result_buffer.append(res)
                                # 2. Ghi ngay xu·ªëng ƒëƒ©a (WAL)
                                self._append_to_temp_file(res)
                        else:
                            self.log_failed_id(pid)

                    # 3. Ki·ªÉm tra buffer xem ƒë·ªß 1000 ch∆∞a
                    while len(result_buffer) >= 1000:
                        batch_to_save = result_buffer[:1000]
                        filename = f"{batch_counter:03d}"
                        self.save_batch(batch_to_save, filename)
                        
                        # --- RICH PROGRESS NOTIFICATION (Every 5 batches) ---
                        if DISCORD_WEBHOOK_URL and (batch_counter % 5 == 0 or batch_counter == 1):
                            elapsed = time.time() - start_time
                            
                            # Metrics Calculation
                            items_done_session = (batch_counter - initial_batch + 1) * 1000
                            avg_speed = items_done_session / elapsed if elapsed > 0 else 0
                            
                            remaining_items = total_source - processed_count - items_done_session
                            eta_min = (remaining_items / avg_speed) / 60 if avg_speed > 0 else 0
                            
                            # Progress Bar
                            current_total = processed_count + items_done_session
                            pct = min(100, int(current_total / total_source * 100))
                            bar_len = 10
                            filled = int(pct / 10)
                            bar = "‚ñì" * filled + "‚ñë" * (bar_len - filled)
                            
                            embed_prog = {
                                "title": f"üöÄ TI·∫æN ƒê·ªò: BATCH {batch_counter}",
                                "color": 3447003, # Blue
                                "fields": [
                                    {"name": "üìà Ti·∫øn ƒë·ªô", "value": f"`[{bar}]` **{pct}%**", "inline": False},
                                    {"name": "‚ö° T·ªëc ƒë·ªô", "value": f"**{avg_speed:.1f}** item/s", "inline": True},
                                    {"name": "‚è±Ô∏è ETA (D·ª± ki·∫øn)", "value": f"~ {eta_min:.1f} ph√∫t", "inline": True},
                                    {"name": "üì¶ M·ªõi t·∫£i", "value": f"{items_done_session:,} sp", "inline": True}
                                ]
                            }
                            await send_discord_webhook(DISCORD_WEBHOOK_URL, embed=embed_prog)
                        elif DISCORD_WEBHOOK_URL:
                             # Log nh·∫π nh√†ng cho c√°c batch l·∫ª
                             self.logger.info(f"üíæ Saved Batch {batch_counter} (Silent)")

                        # C·∫Øt buffer v√† update l·∫°i file nh√°p
                        result_buffer = result_buffer[1000:]
                        self._rewrite_temp_file(result_buffer)
                        
                        batch_counter += 1
            
            except asyncio.CancelledError:
                self.logger.warning("‚ö†Ô∏è Task b·ªã h·ªßy (Ctrl+C)!")
                embed_stop = {
                    "title": "‚ö†Ô∏è CRAWLER STOPPED",
                    "description": "User ƒë√£ d·ª´ng th·ªß c√¥ng (Ctrl+C).",
                    "color": 16776960 # Yellow
                }
                await send_discord_webhook(DISCORD_WEBHOOK_URL, embed=embed_stop)
                raise
            except Exception as e:
                self.logger.error(f"‚ùå L·ªói kh√¥ng mong mu·ªën trong loop: {e}")
                embed_err = {
                    "title": "‚ùå CRAWLER CRASHED!",
                    "description": f"L·ªói nghi√™m tr·ªçng: `{str(e)}`",
                    "color": 15158332 # Red
                }
                await send_discord_webhook(DISCORD_WEBHOOK_URL, embed=embed_err)
                raise
            finally:
                # L∆∞u n·ªët ph·∫ßn d∆∞ c√≤n l·∫°i trong buffer ra file JSON lu√¥n (thay v√¨ ch·ªâ ƒë·ªÉ trong WAL)
                if result_buffer:
                    self.logger.info(f"üíæ GRACEFUL SHUTDOWN: L∆∞u n·ªët {len(result_buffer)} s·∫£n ph·∫©m cu·ªëi c√πng v√†o file...")
                    self.save_batch(result_buffer, f"{batch_counter:03d}")
            
            # --- RICH FINISH NOTIFICATION ---
            elapsed = time.time() - start_time
            # ∆Ø·ªõc l∆∞·ª£ng s·∫£n ph·∫©m ƒë√£ ch·∫°y trong session n√†y
            final_session_items = (batch_counter - initial_batch) * 1000 
            if result_buffer: final_session_items += len(result_buffer) # Add buffer if any

            if DISCORD_WEBHOOK_URL:
                 embed_finish = {
                    "title": "‚úÖ CRAWLER HO√ÄN TH√ÄNH!",
                    "description": "To√†n b·ªô d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c t·∫£i v·ªÅ an to√†n.",
                    "color": 3066993, # Green
                    "fields": [
                        {"name": "‚è±Ô∏è T·ªïng th·ªùi gian", "value": f"{elapsed/60:.1f} ph√∫t", "inline": True},
                        {"name": "üì¶ T·ªïng s·∫£n ph·∫©m (Session)", "value": f"{final_session_items:,}", "inline": True},
                        {"name": "üî• Tr·∫°ng th√°i", "value": "S·∫µn s√†ng Ingest DB", "inline": False}
                    ]
                }
                 await send_discord_webhook(DISCORD_WEBHOOK_URL, embed=embed_finish)
            
            msg_finish = "‚úÖ **Tiki Crawler**: HO√ÄN TH√ÄNH TO√ÄN B·ªò C√îNG VI·ªÜC."
            self.logger.info(msg_finish.replace("**", ""))

    def log_failed_id(self, product_id):
        """Ghi ID b·ªã l·ªói v√†o file ri√™ng ƒë·ªÉ retry sau"""
        file_path = os.path.join(self.log_dir, "failed_products.txt")
        with open(file_path, "a") as f:
            f.write(f"{product_id}\n")
