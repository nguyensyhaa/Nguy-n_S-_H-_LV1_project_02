
import argparse
import asyncio
import sys
import os
import csv
import json
import glob
from .pipeline import TikiPipeline
from .etl import run_etl_pipeline
import logging

def cmd_crawl(args):
    """Lá»‡nh cÃ o dá»¯ liá»‡u"""
    input_file = args.input
    if not os.path.exists(input_file):
        print(f"âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y file input '{input_file}'")
        return

    print(f"ğŸš€ Báº¯t Ä‘áº§u crawl tá»« file: {input_file}")
    pipeline = TikiPipeline(input_file=input_file)
    try:
        asyncio.run(pipeline.run())
    except KeyboardInterrupt:
        print("\nâš ï¸ ÄÃ£ dá»«ng thá»§ cÃ´ng (Ctrl+C).")
    except Exception as e:
        print(f"âŒ Lá»—i khÃ´ng mong muá»‘n: {e}")

def cmd_validate(args):
    """Lá»‡nh kiá»ƒm tra input"""
    input_file = args.input
    if not os.path.exists(input_file):
        print(f"âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y file '{input_file}'")
        return

    print(f"ğŸ” Äang kiá»ƒm tra file: {input_file} ...")
    unique_ids = set()
    duplicates = []
    invalid_ids = []
    total_rows = 0

    try:
        with open(input_file, mode='r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            if 'id' not in reader.fieldnames:
                print(f"âŒ Lá»–I: File CSV thiáº¿u cá»™t 'id'. Found: {reader.fieldnames}")
                return

            for row in reader:
                total_rows += 1
                raw_id = row.get('id', '').strip()
                if not raw_id: continue
                if not raw_id.isdigit():
                    invalid_ids.append(raw_id)
                    continue
                
                if raw_id in unique_ids:
                    duplicates.append(raw_id)
                else:
                    unique_ids.add(raw_id)
                    
        print("-" * 30)
        print(f"âœ… HoÃ n thÃ nh kiá»ƒm tra!")
        print(f"â€¢ Tá»•ng dÃ²ng: {total_rows}")
        print(f"â€¢ ID há»£p lá»‡: {len(unique_ids)}")
        print(f"â€¢ ID trÃ¹ng láº·p: {len(duplicates)}")
        print(f"â€¢ ID lá»—i format: {len(invalid_ids)}")
        
        if duplicates:
            print(f"âš ï¸ Cáº¢NH BÃO: CÃ³ {len(duplicates)} ID trÃ¹ng láº·p.")
        else:
            print("âœ¨ Dá»¯ liá»‡u sáº¡ch, khÃ´ng trÃ¹ng láº·p.")
            
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c file: {e}")

def cmd_merge(args):
    """Lá»‡nh gá»™p file JSON"""
    data_dir = args.data_dir
    output_file = args.output
    
    print(f"â³ Äang gá»™p dá»¯ liá»‡u tá»« '{data_dir}' vÃ o '{output_file}'...")
    all_data = []
    pattern = os.path.join(data_dir, "products_batch_*.json")
    files = glob.glob(pattern)
    
    if not files:
        print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file batch nÃ o trong {data_dir}")
        return

    for f_path in files:
        try:
            with open(f_path, 'r', encoding='utf-8') as f:
                batch_data = json.load(f)
                all_data.extend(batch_data)
        except Exception as e:
            print(f"âš ï¸ Bá» qua file lá»—i {f_path}: {e}")

    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ÄÃƒ XONG! Gá»™p {len(files)} file thÃ nh cÃ´ng.")
        print(f"ğŸ“ Tá»•ng cá»™ng: {len(all_data)} sáº£n pháº©m.")
    except Exception as e:
        print(f"âŒ Lá»—i ghi file output: {e}")

def cmd_ingest(args):
    """Lá»‡nh Ingest dá»¯ liá»‡u vÃ o Postgres"""
    data_dir = args.data_dir
    pattern = os.path.join(data_dir, "products_batch_*.json")
    files = glob.glob(pattern)
    
    if not files:
        print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file batch nÃ o trong {data_dir}")
        return

    print(f"ğŸš€ Báº¯t Ä‘áº§u Ingest {len(files)} files vÃ o Database...")
    
    # Setup basic console logging for the user to see progress
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    for i, f_path in enumerate(files, 1):
        print(f"[{i}/{len(files)}] Processing {os.path.basename(f_path)}...")
        try:
            run_etl_pipeline(f_path)
        except Exception as e:
             print(f"âŒ Failed to ingest {f_path}: {e}")
    
    print("âœ… Ingest hoÃ n táº¥t.")

def cmd_retry(args):
    """Lá»‡nh thá»­ láº¡i cÃ¡c ID bá»‹ lá»—i (404/Failed)"""
    log_file = args.log_file
    if not os.path.exists(log_file):
        print(f"âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y file log '{log_file}'")
        return

    print(f"ğŸ”„ Äang Ä‘á»c ID lá»—i tá»«: {log_file} ...")
    retry_ids = []
    try:
        with open(log_file, 'r') as f:
            for line in f:
                clean_id = line.strip()
                if clean_id.isdigit():
                    retry_ids.append(clean_id)
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c file log: {e}")
        return

    if not retry_ids:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y ID nÃ o Ä‘á»ƒ retry.")
        return

    print(f"ğŸ”¥ TÃ¬m tháº¥y {len(retry_ids)} ID cáº§n thá»­ láº¡i.")
    
    # Táº¡o file input táº¡m thá»i
    temp_input = "temp_retry_input.csv"
    try:
        with open(temp_input, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['id'])
            for rid in retry_ids:
                writer.writerow([rid])
        
        print(f"ğŸ“ ÄÃ£ táº¡o file input táº¡m: {temp_input}")
        print("-" * 40)
        
        # Cháº¡y Pipeline
        pipeline = TikiPipeline(input_file=temp_input)
        asyncio.run(pipeline.run())
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ÄÃ£ dá»«ng thá»§ cÃ´ng (Ctrl+C).")
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")
    finally:
        if os.path.exists(temp_input):
            os.remove(temp_input)
            print(f"ğŸ—‘ï¸ ÄÃ£ xÃ³a file táº¡m: {temp_input}")


def main():
    parser = argparse.ArgumentParser(description="Tiki Scraper Tool - High Performance Crawler")
    subparsers = parser.add_subparsers(dest="command", help="Lá»‡nh cáº§n cháº¡y")

    # Command: crawl
    crawl_parser = subparsers.add_parser("crawl", help="Báº¯t Ä‘áº§u cÃ o dá»¯ liá»‡u")
    crawl_parser.add_argument("--input", "-i", required=True, help="ÄÆ°á»ng dáº«n file CSV input")
    crawl_parser.set_defaults(func=cmd_crawl)

    # Command: validate
    validate_parser = subparsers.add_parser("validate", help="Kiá»ƒm tra file input CSV")
    validate_parser.add_argument("--input", "-i", required=True, help="ÄÆ°á»ng dáº«n file CSV input")
    validate_parser.set_defaults(func=cmd_validate)

    # Command: merge
    merge_parser = subparsers.add_parser("merge", help="Gá»™p cÃ¡c file JSON thÃ nh 1 file lá»›n")
    merge_parser.add_argument("--data-dir", "-d", default="data", help="ThÆ° má»¥c chá»©a file batch (default: data)")
    merge_parser.add_argument("--output", "-o", default="all_products.json", help="File output (default: all_products.json)")
    merge_parser.set_defaults(func=cmd_merge)

    # Command: ingest
    ingest_parser = subparsers.add_parser("ingest", help="Ingest dá»¯ liá»‡u JSON vÃ o Postgres")
    ingest_parser.add_argument("--data-dir", "-d", default="data", help="ThÆ° má»¥c chá»©a file batch (default: data)")
    ingest_parser.set_defaults(func=cmd_ingest)

    # Command: retry
    retry_parser = subparsers.add_parser("retry", help="Thá»­ láº¡i cÃ¡c ID bá»‹ lá»—i")
    retry_parser.add_argument("--log-file", "-l", default="logs/failed_products.txt", help="File chá»©a danh sÃ¡ch ID lá»—i (default: logs/failed_products.txt)")
    retry_parser.set_defaults(func=cmd_retry)

    args = parser.parse_args()
    
    if args.command:
        args.func(args)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
