
import argparse
import asyncio
import sys
import os
import csv
import json
import glob
from .pipeline import TikiPipeline

def cmd_crawl(args):
    """Lá»‡nh cÃ o dá»¯ liá»‡u"""
    input_file = args.input
    if not os.path.exists(input_file):
        print(f"âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y file input '{input_file}'")
        return

    print(f"ğŸš€ Báº¯t Ä‘áº§u crawl tá»« file: {input_file}")
    pipeline = TikiPipeline(input_file=input_file)
    try:
        asyncio.run(pipeline.run())
    except KeyboardInterrupt:
        print("\nâš ï¸ ÄÃ£ dá»«ng thá»§ cÃ´ng (Ctrl+C).")
    except Exception as e:
        print(f"âŒ Lá»—i khÃ´ng mong muá»‘n: {e}")

def cmd_validate(args):
    """Lá»‡nh kiá»ƒm tra input"""
    input_file = args.input
    if not os.path.exists(input_file):
        print(f"âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y file '{input_file}'")
        return

    print(f"ğŸ” Äang kiá»ƒm tra file: {input_file} ...")
    unique_ids = set()
    duplicates = []
    invalid_ids = []
    total_rows = 0

    try:
        with open(input_file, mode='r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            if 'id' not in reader.fieldnames:
                print(f"âŒ Lá»–I: File CSV thiáº¿u cá»™t 'id'. Found: {reader.fieldnames}")
                return

            for row in reader:
                total_rows += 1
                raw_id = row.get('id', '').strip()
                if not raw_id: continue
                if not raw_id.isdigit():
                    invalid_ids.append(raw_id)
                    continue
                
                if raw_id in unique_ids:
                    duplicates.append(raw_id)
                else:
                    unique_ids.add(raw_id)
                    
        print("-" * 30)
        print(f"âœ… HoÃ n thÃ nh kiá»ƒm tra!")
        print(f"â€¢ Tá»•ng dÃ²ng: {total_rows}")
        print(f"â€¢ ID há»£p lá»‡: {len(unique_ids)}")
        print(f"â€¢ ID trÃ¹ng láº·p: {len(duplicates)}")
        print(f"â€¢ ID lá»—i format: {len(invalid_ids)}")
        
        if duplicates:
            print(f"âš ï¸ Cáº¢NH BÃO: CÃ³ {len(duplicates)} ID trÃ¹ng láº·p.")
        else:
            print("âœ¨ Dá»¯ liá»‡u sáº¡ch, khÃ´ng trÃ¹ng láº·p.")
            
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c file: {e}")

def cmd_merge(args):
    """Lá»‡nh gá»™p file JSON"""
    data_dir = args.data_dir
    output_file = args.output
    
    print(f"â³ Äang gá»™p dá»¯ liá»‡u tá»« '{data_dir}' vÃ o '{output_file}'...")
    all_data = []
    pattern = os.path.join(data_dir, "products_batch_*.json")
    files = glob.glob(pattern)
    
    if not files:
        print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file batch nÃ o trong {data_dir}")
        return

    for f_path in files:
        try:
            with open(f_path, 'r', encoding='utf-8') as f:
                batch_data = json.load(f)
                all_data.extend(batch_data)
        except Exception as e:
            print(f"âš ï¸ Bá» qua file lá»—i {f_path}: {e}")

    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ÄÃƒ XONG! Gá»™p {len(files)} file thÃ nh cÃ´ng.")
        print(f"ğŸ“ Tá»•ng cá»™ng: {len(all_data)} sáº£n pháº©m.")
    except Exception as e:
        print(f"âŒ Lá»—i ghi file output: {e}")

def main():
    parser = argparse.ArgumentParser(description="Tiki Scraper Tool - High Performance Crawler")
    subparsers = parser.add_subparsers(dest="command", help="Lá»‡nh cáº§n cháº¡y")

    # Command: crawl
    crawl_parser = subparsers.add_parser("crawl", help="Báº¯t Ä‘áº§u cÃ o dá»¯ liá»‡u")
    crawl_parser.add_argument("--input", "-i", required=True, help="ÄÆ°á»ng dáº«n file CSV input")
    crawl_parser.set_defaults(func=cmd_crawl)

    # Command: validate
    validate_parser = subparsers.add_parser("validate", help="Kiá»ƒm tra file input CSV")
    validate_parser.add_argument("--input", "-i", required=True, help="ÄÆ°á»ng dáº«n file CSV input")
    validate_parser.set_defaults(func=cmd_validate)

    # Command: merge
    merge_parser = subparsers.add_parser("merge", help="Gá»™p cÃ¡c file JSON thÃ nh 1 file lá»›n")
    merge_parser.add_argument("--data-dir", "-d", default="data", help="ThÆ° má»¥c chá»©a file batch (default: data)")
    merge_parser.add_argument("--output", "-o", default="all_products.json", help="File output (default: all_products.json)")
    merge_parser.set_defaults(func=cmd_merge)

    args = parser.parse_args()
    
    if args.command:
        args.func(args)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
